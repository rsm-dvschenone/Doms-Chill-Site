{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multinomial Logit Model\"\n",
        "author: \"Dominic Schenone\"\n",
        "date: \"May 28 2025\"\n",
        "callout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n",
        "execute:\n",
        "  warning: false\n",
        "  message: false\n",
        "---\n",
        "\n",
        "\n",
        "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data.\n",
        "\n",
        ":::: {.callout-note collapse=\"true\"}"
      ],
      "id": "c99b2105"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Define attributes\n",
        "brands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\n",
        "ads = [\"Yes\", \"No\"]\n",
        "prices = np.arange(8, 33, 4)\n",
        "\n",
        "# Generate all possible profiles\n",
        "profiles = pd.DataFrame([(b, a, p) for b in brands for a in ads for p in prices],\n",
        "                        columns=[\"brand\", \"ad\", \"price\"])\n",
        "\n",
        "# Utility functions\n",
        "b_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\n",
        "a_util = {\"Yes\": -0.8, \"No\": 0.0}\n",
        "p_util = lambda p: -0.1 * p\n",
        "\n",
        "# Simulation parameters\n",
        "n_peeps = 100\n",
        "n_tasks = 10\n",
        "n_alts = 3\n",
        "\n",
        "# Simulate one respondent\n",
        "def sim_one(id):\n",
        "    datlist = []\n",
        "    for t in range(1, n_tasks + 1):\n",
        "        sampled = profiles.sample(n=n_alts, replace=False).copy()\n",
        "        sampled.insert(0, \"task\", t)\n",
        "        sampled.insert(0, \"resp\", id)\n",
        "        sampled[\"v\"] = (\n",
        "            sampled[\"brand\"].map(b_util) +\n",
        "            sampled[\"ad\"].map(a_util) +\n",
        "            p_util(sampled[\"price\"])\n",
        "        ).round(10)\n",
        "        sampled[\"e\"] = -np.log(-np.log(np.random.rand(n_alts)))\n",
        "        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n",
        "        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n",
        "        datlist.append(sampled)\n",
        "    return pd.concat(datlist, ignore_index=True)\n",
        "\n",
        "# Generate data\n",
        "conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n",
        "\n",
        "# Keep only observable variables\n",
        "conjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]"
      ],
      "id": "29432722",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n",
        "\n",
        "\n",
        "To prepare the data:\n",
        "- We created **dummy variables** for the `brand` attribute. Specifically, we included binary indicators for **Netflix** and **Prime Video**, using **Hulu** as the baseline.\n",
        "- The `ad` feature was converted into a **binary variable**, where 1 indicates the presence of ads and 0 indicates an ad-free option.\n",
        "- We retained the `price` and `choice` columns in their original form.\n",
        "\n",
        "Below is the Python code used for data preprocessing. The previewed table shows that each row corresponds to one product alternative shown in a choice task.\n"
      ],
      "id": "24fcda4f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading and and preprocessing of  data\n",
        "df = pd.read_csv('conjoint_data.csv')\n",
        "df = pd.get_dummies(df, columns=[\"brand\"], drop_first=True)\n",
        "df[\"ad\"] = df[\"ad\"].map({\"Yes\": 1, \"No\": 0})\n",
        "df.rename(columns={\"brand_N\": \"netflix\", \"brand_P\": \"prime\"}, inplace=True)\n",
        "df.head()\n",
        "\n",
        "# Create design matrix and response vector\n",
        "\n",
        "X_columns = [\"netflix\", \"prime\", \"ad\", \"price\"]\n",
        "X = df[X_columns].values\n",
        "y = df[\"choice\"].values\n",
        "df[\"choice_set\"] = df[\"resp\"].astype(str) + \"_\" + df[\"task\"].astype(str)\n",
        "choice_sets = df[\"choice_set\"].values"
      ],
      "id": "49d4599a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Estimation via Maximum Likelihood\n",
        "\n",
        "We begin by defining the log-likelihood function for the Multinomial Logit (MNL) model. For each choice set (i.e., a group of three product alternatives shown to a respondent), we compute the probability of the selected alternative using the softmax function applied to the utility values. These utilities are linear combinations of the feature values and coefficients (betas).\n",
        "\n",
        "The function below returns the **negative log-likelihood**, which we will minimize using `scipy.optimize`."
      ],
      "id": "3978911d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def log_likelihood(beta, X, y, groups):\n",
        "    log_lik = 0\n",
        "    beta = np.array(beta)\n",
        "    for group in np.unique(groups):\n",
        "        idx = groups == group\n",
        "        X_group = X[idx]\n",
        "        y_group = y[idx]\n",
        "\n",
        "        utilities = np.dot(X_group, beta)\n",
        "        utilities = np.asarray(utilities, dtype=np.float64)  # <- Force to numpy float64\n",
        "\n",
        "        exp_utilities = np.exp(utilities - np.max(utilities))  # numerical stability\n",
        "        probs = exp_utilities / np.sum(exp_utilities)\n",
        "\n",
        "        log_lik += np.log(probs[y_group == 1][0])\n",
        "\n",
        "    return -log_lik\n"
      ],
      "id": "6dcf2bce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "\n",
        "# Start with zero coefficients\n",
        "initial_beta = np.zeros(X.shape[1])\n",
        "\n",
        "# Minimize the negative log-likelihood\n",
        "result = minimize(\n",
        "    log_likelihood,\n",
        "    initial_beta,\n",
        "    args=(X, y, choice_sets),\n",
        "    method=\"BFGS\",\n",
        "    options={\"disp\": True}\n",
        ")\n",
        "\n",
        "# Estimated coefficients\n",
        "beta_hat = result.x\n",
        "\n",
        "# Standard errors from inverse Hessian\n",
        "hessian_inv = result.hess_inv\n",
        "se = np.sqrt(np.diag(hessian_inv))\n",
        "\n",
        "# 95% confidence intervals\n",
        "z = 1.96\n",
        "conf_int = np.array([beta_hat - z * se, beta_hat + z * se]).T\n",
        "\n",
        "# Combining everything into a table\n",
        "import pandas as pd\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Estimate\": beta_hat,\n",
        "    \"Std. Error\": se,\n",
        "    \"95% CI Lower\": conf_int[:, 0],\n",
        "    \"95% CI Upper\": conf_int[:, 1]\n",
        "}, index=[\"netflix\", \"prime\", \"ad\", \"price\"])\n"
      ],
      "id": "7aef4a49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We estimated the parameters of the multinomial logit model using the BFGS algorithm via `scipy.optimize.minimize()`, minimizing the negative log-likelihood function defined earlier. The model includes binary indicators for `netflix`, `prime`, and `ad`, with `hulu` and `no ads` as the respective baseline categories, along with the continuous variable `price`.\n",
        "\n",
        "The table below summarizes the parameter estimates, standard errors (from the inverse Hessian), and 95% confidence intervals:"
      ],
      "id": "8d9a8b79"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summary_df.round(3)"
      ],
      "id": "f8bc1a6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note icon=\"ðŸ§ \"}\n",
        "\n",
        "Interpreting the Estimates\n",
        "\n",
        "- Brand Preference: Netflix and Prime are both preferred over the baseline (Hulu), with Netflix receiving the highest utility.\n",
        "\n",
        "- Ad Dislike: The negative coefficient for ad confirms that respondents significantly prefer ad-free experiences.\n",
        "\n",
        "- Price Sensitivity: The negative and significant coefficient for price aligns with economic intuition â€” as price increases, utility decreases.\n",
        ":::\n",
        "\n",
        "\n",
        "## 5. Estimation via Bayesian Methods\n",
        "\n",
        "_todo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000._"
      ],
      "id": "11197b61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Log-prior function\n",
        "def log_prior(beta):\n",
        "    # Priors: N(0,5) for all except price (N(0,1))\n",
        "    prior = -0.5 * ((beta[0]/np.sqrt(5))**2 +\n",
        "                    (beta[1]/np.sqrt(5))**2 +\n",
        "                    (beta[2]/np.sqrt(5))**2 +\n",
        "                    (beta[3]/1)**2)\n",
        "    return prior\n",
        "\n",
        "# Log-posterior function\n",
        "def log_posterior(beta, X, y, groups):\n",
        "    return -log_likelihood(beta, X, y, groups) + log_prior(beta)\n",
        "\n",
        "# MCMC settings\n",
        "n_steps = 11000\n",
        "burn_in = 1000\n",
        "n_params = X.shape[1]\n",
        "\n",
        "# Proposal distribution: diagonal covariances\n",
        "proposal_sds = [0.05, 0.05, 0.05, 0.005]\n",
        "\n",
        "# Storage\n",
        "samples = np.zeros((n_steps, n_params))\n",
        "beta_current = np.zeros(n_params)\n",
        "log_post_current = log_posterior(beta_current, X, y, choice_sets)\n",
        "\n",
        "# Metropolis-Hastings loop\n",
        "for step in range(n_steps):\n",
        "    # Propose new beta from independent normal perturbations\n",
        "    beta_proposal = beta_current + np.random.normal(0, proposal_sds)\n",
        "\n",
        "    # Compute log-posterior\n",
        "    log_post_proposal = log_posterior(beta_proposal, X, y, choice_sets)\n",
        "\n",
        "    # Acceptance probability\n",
        "    accept_prob = min(1, np.exp(log_post_proposal - log_post_current))\n",
        "\n",
        "    # Accept or reject\n",
        "    if np.random.rand() < accept_prob:\n",
        "        beta_current = beta_proposal\n",
        "        log_post_current = log_post_proposal\n",
        "\n",
        "    samples[step] = beta_current"
      ],
      "id": "2ec10749",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_hint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta._\n",
        "\n",
        "_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\n",
        "\n",
        "_hint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands.  Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous.  So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal.  Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005)._\n",
        "\n",
        "\n",
        "_todo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution._"
      ],
      "id": "9cb75fec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assume 'samples' is your MCMC output from earlier\n",
        "# Retain only the last 10,000 samples (after 1,000 burn-in)\n",
        "posterior_samples = samples[1000:]\n",
        "\n",
        "# Posterior summaries\n",
        "posterior_means = posterior_samples.mean(axis=0)\n",
        "posterior_stds = posterior_samples.std(axis=0)\n",
        "z = 1.96\n",
        "posterior_ci = np.array([\n",
        "    posterior_means - z * posterior_stds,\n",
        "    posterior_means + z * posterior_stds\n",
        "]).T\n",
        "\n",
        "# Create summary table\n",
        "posterior_summary = pd.DataFrame({\n",
        "    \"Posterior Mean\": posterior_means,\n",
        "    \"Posterior SD\": posterior_stds,\n",
        "    \"95% CI Lower\": posterior_ci[:, 0],\n",
        "    \"95% CI Upper\": posterior_ci[:, 1]\n",
        "}, index=[\"netflix\", \"prime\", \"ad\", \"price\"])\n",
        "\n",
        "posterior_summary.round(3)"
      ],
      "id": "7149cc41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip title=\"Posterior Distribution Interpretations\" collapse=\"true\"}\n",
        "The table above summarizes the posterior distributions for the four model parameters. The posterior means provide point estimates, while the standard deviations and 95% credible intervals reflect uncertainty from the Bayesian perspective.\n",
        "\n",
        "- **Netflix** has the highest posterior mean (0.88), indicating that, all else equal, consumers strongly prefer Netflix over the baseline (Hulu). The 95% credible interval [0.36, 1.40] confirms this preference is statistically meaningful.\n",
        "- **Prime Video** is also preferred over Hulu, though to a lesser extent (posterior mean 0.47).\n",
        "- The presence of **ads** has a large negative impact on utility, with a posterior mean of -0.69 and a wide 95% credible interval that remains below zero.\n",
        "- The **price** coefficient is negative as expected (mean = -0.093), and the tight credible interval [âˆ’0.144, âˆ’0.042] reflects strong evidence for price sensitivity.\n",
        "\n",
        "These results align closely with the Maximum Likelihood estimates obtained earlier, offering a robust confirmation of the preference structure under both frequentist and Bayesian approaches.\n",
        ":::\n",
        "\n",
        "_todo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach._"
      ],
      "id": "8b188909"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot for Î²_price (index = 3)\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 6))\n",
        "\n",
        "axs[0].plot(posterior_samples[:, 3])\n",
        "axs[0].set_title(\"Trace Plot for Î²_price\")\n",
        "axs[0].set_xlabel(\"Iteration\")\n",
        "axs[0].set_ylabel(\"Value\")\n",
        "\n",
        "axs[1].hist(posterior_samples[:, 3], bins=30, density=True)\n",
        "axs[1].set_title(\"Posterior Distribution for Î²_price\")\n",
        "axs[1].set_xlabel(\"Î²_price\")\n",
        "axs[1].set_ylabel(\"Density\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ff0f4295",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note title=\"Plot Interpretation\" collapse=\"true\"}\n",
        "The trace plot and posterior histogram for $\\beta_{\\text{price}}$ provide insight into the behavior and stability of the MCMC sampler.\n",
        "\n",
        "- The **trace plot** shows that the chain is generally well-mixed and stationary across most of the 10,000 samples, indicating that the sampler is exploring the posterior effectively. However, the spike at the very end suggests an unusually high proposed value that was accepted â€” likely due to a favorable random draw. While rare, these jumps can happen in high-probability tails, especially when using independent Gaussian proposals.\n",
        "- The **posterior histogram** is centered around -0.093 and closely resembles a normal distribution. The shape and narrow spread of the distribution reinforce the conclusion that consumers have a consistently negative sensitivity to price.\n",
        "\n",
        "Despite the spike, the overall trace indicates **good mixing and convergence**, and the posterior inference remains valid. If this were production-grade work, one could re-run with a longer chain or multiple chains to confirm robustness.\n",
        ":::\n",
        "\n",
        "\n",
        "## 6. Discussion\n",
        "\n",
        "\n",
        "\n",
        "Had we not simulated the data ourselves, we would still be able to draw clear insights from the estimated parameters. The results from both the MLE and Bayesian methods point to consistent and interpretable consumer preferences:\n",
        "\n",
        "- The fact that $\\beta_{\\text{Netflix}} > \\beta_{\\text{Prime}}$ suggests that consumers **prefer Netflix over Prime Video**, all else equal. Since Hulu was used as the baseline, both streaming services are valued more than Hulu, but Netflix stands out as the strongest brand in terms of utility.\n",
        "- The negative value of $\\beta_{\\text{price}}$ makes intuitive economic sense. A higher price reduces the utility of a product, and this is reflected consistently in the estimates across all methods. This negative relationship is critical to capture when predicting consumer choice or conducting pricing strategy simulations.\n",
        "\n",
        "### Moving Toward Real-World Conjoint Models\n",
        "\n",
        "In practice, consumer preferences are rarely fixed. Instead, they vary across individuals. A standard multinomial logit model assumes a **single set of utility weights shared by all consumers**, which is unrealistic in many real-world contexts. To account for **heterogeneity in preferences**, we would move to a:\n",
        "\n",
        "- **Multi-level model** (also known as a **random-parameter** or **hierarchical Bayes model**)\n",
        "\n",
        "At a high level, this approach involves:\n",
        "- Simulating data by allowing each individual to have their **own set of $\\beta$ coefficients**, drawn from a higher-level distribution (e.g., $\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)$).\n",
        "- Estimating both the **individual-level parameters** and the **population-level distribution** of preferences using hierarchical Bayesian methods or mixed logit models.\n",
        "\n",
        "These models are more computationally intensive, but they provide richer, more personalized insights and are better suited for targeting, segmentation, and prediction in real-world marketing and product design contexts.\n"
      ],
      "id": "7dfe2f4e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/conda/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}