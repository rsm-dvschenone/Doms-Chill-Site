[
  {
    "objectID": "project_assignments/HW2/index.html",
    "href": "project_assignments/HW2/index.html",
    "title": "HW2",
    "section": "",
    "text": "This is where I will complete hw 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dominic Schenone",
    "section": "",
    "text": "This is the beginning of my Quarto Website. I will continue to use this\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Dom’s Resume",
    "section": "",
    "text": "Last updated: 9/1/2024\nDownload PDF file."
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Dom’s Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\nDominic Schenone\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nDominic Schenone\nMay 11, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project_assignments/HW1/hw1_questions.html",
    "href": "project_assignments/HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate the key findings from Karlan and List (2007), explore extensions of their analysis, and reflect on the implications of their design and results. Throughout, I aim to verify treatment effects on donation rates and amounts, assess baseline balance, and explore how match ratios influence giving behavior."
  },
  {
    "objectID": "project_assignments/HW1/hw1_questions.html#introduction",
    "href": "project_assignments/HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate the key findings from Karlan and List (2007), explore extensions of their analysis, and reflect on the implications of their design and results. Throughout, I aim to verify treatment effects on donation rates and amounts, assess baseline balance, and explore how match ratios influence giving behavior."
  },
  {
    "objectID": "project_assignments/HW1/hw1_questions.html#data",
    "href": "project_assignments/HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\n##Importing Pandas and also our data as df \nimport pandas as pd \ndf = pd.read_stata('karlan_list_2007.dta')\n\n##Looking at some of the qualities of the data\n##The Shape\nprint(\"The Shape of our Data: \", df.shape)\nprint(\"The Different DataTypes of our columns\")\n\nprint(\"Info: \", df.info())\nprint(\"First 5 Rows\", df.head())\nprint(\"DataTypes of columns: \", df.dtypes)\n\nThe Shape of our Data:  (50083, 51)\nThe Different DataTypes of our columns\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\nInfo:  None\nFirst 5 Rows    treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\nDataTypes of columns:  treatment                 int8\ncontrol                   int8\nratio                 category\nratio2                    int8\nratio3                    int8\nsize                  category\nsize25                    int8\nsize50                    int8\nsize100                   int8\nsizeno                    int8\nask                   category\naskd1                     int8\naskd2                     int8\naskd3                     int8\nask1                     int16\nask2                     int16\nask3                     int16\namount                 float32\ngave                      int8\namountchange           float32\nhpa                    float32\nltmedmra                  int8\nfreq                     int16\nyears                  float64\nyear5                     int8\nmrm2                   float64\ndormant                   int8\nfemale                 float64\ncouple                 float64\nstate50one                int8\nnonlit                 float64\ncases                  float64\nstatecnt               float32\nstateresponse          float32\nstateresponset         float32\nstateresponsec         float32\nstateresponsetminc     float32\nperbush                float32\nclose25                float64\nred0                   float64\nblue0                  float64\nredcty                 float64\nbluecty                float64\npwhite                 float32\npblack                 float32\npage18_39              float32\nave_hh_sz              float32\nmedian_hhincome        float64\npowner                 float32\npsch_atlstba           float32\npop_propurban          float32\ndtype: object\n\n\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n#These are the variables that I am going to include\n#They are not outcome variables, so they will be good to use to test to see if the population distrbutions are significantly different:\n#mrm2 – months since last donation\n#hpa – highest previous amount donated\n#freq – number of prior donations\nfrom scipy.stats import t\nimport numpy as np\n\ndef compute_ci(var, group_var=\"treatment\", conf_level=0.95):\n    a = df[df[group_var] == 1][var].dropna()\n    b = df[df[group_var] == 0][var].dropna()\n\n    x_a, x_b = a.mean(), b.mean()\n    s_a, s_b = a.std(), b.std()\n    n_a, n_b = len(a), len(b)\n\n    diff = x_a - x_b\n    se = np.sqrt((s_a**2 / n_a) + (s_b**2 / n_b))\n    dof = min(n_a, n_b) - 1\n    t_star = t.ppf((1 + conf_level) / 2, df=dof)\n\n    margin = t_star * se\n    lower, upper = diff - margin, diff + margin\n\n    print(f\"{var} | Mean Diff: {diff:.4f} | CI: ({lower:.4f}, {upper:.4f})\")\n\n# Run CI calculations for a few variables\nfor variable in [\"mrm2\", \"hpa\", \"freq\"]:\n    compute_ci(variable)\n\nmrm2 | Mean Diff: 0.0137 | CI: (-0.2107, 0.2381)\nhpa | Mean Diff: 0.6371 | CI: (-0.6498, 1.9239)\nfreq | Mean Diff: -0.0120 | CI: (-0.2238, 0.1998)\n\n\n\n\n\n\n\n\nBalance Check Interpretation\n\n\n\nTo assess balance across treatment and control groups, I computed 95% confidence intervals for the difference in means on several baseline characteristics. The intervals for mrm2, hpa, and freq all include zero, suggesting that the groups were well-balanced and no statistically significant differences were present. This supports the validity of the random assignment.\n\n\nNow I will go on to see if the results from making a linear regression, for variable mrm2, will match that from that as the t-test @95% confidence\n\nfrom scipy.stats import t\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# --- T-Test (using classic formula) ---\na = df[df[\"treatment\"] == 1][\"mrm2\"].dropna()\nb = df[df[\"treatment\"] == 0][\"mrm2\"].dropna()\n\nmean_diff = a.mean() - b.mean()\nse = np.sqrt((a.var(ddof=1)/len(a)) + (b.var(ddof=1)/len(b)))\ndof = min(len(a), len(b)) - 1\nt_stat = mean_diff / se\n\nprint(f\"T-Test result for mrm2:\")\nprint(f\"  Mean difference: {mean_diff:.4f}\")\nprint(f\"  SE: {se:.4f}\")\nprint(f\"  t-stat: {t_stat:.4f}\")\n\n# --- Linear Regression ---\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nprint(\"\\nLinear Regression result for mrm2:\")\nprint(model.summary())\n\nT-Test result for mrm2:\n  Mean difference: 0.0137\n  SE: 0.1145\n  t-stat: 0.1195\n\nLinear Regression result for mrm2:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 07 May 2025   Prob (F-statistic):              0.905\nTime:                        12:51:14   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo verify the balance of baseline variable mrm2, I conducted both a two-sample t-test and a linear regression with treatment as the sole predictor. Both approaches yielded nearly identical estimates:\nMean difference: 0.0137\nStandard error: ~0.115\nt-statistic: 0.119\nThese consistent results confirm that a linear regression with a binary treatment variable is mathematically equivalent to a t-test. This approach mirrors the analysis used in Table 1 of Karlan and List (2007), which demonstrates the success of randomization in balancing observable characteristics across groups. ## Experimental Results\n\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate mean donation rate by group\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"group\"] = donation_rates[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\n# Create barplot\nsns.barplot(data=donation_rates, x=\"group\", y=\"gave\")\nplt.title(\"Proportion of People Who Donated\")\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"Group\")\nplt.ylim(0, donation_rates[\"gave\"].max() + 0.01)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# T-test: donation rates (binary outcome)\ngave_ttest = ttest_ind(df[df[\"treatment\"] == 1][\"gave\"],\n                       df[df[\"treatment\"] == 0][\"gave\"])\nprint(\"T-test result for 'gave' (donation made):\", gave_ttest)\n\n# Bivariate regression: gave ~ treatment\ngave_reg = smf.ols(\"gave ~ treatment\", data=df).fit()\nprint(gave_reg.summary())\n\nT-test result for 'gave' (donation made): TtestResult(statistic=3.101361000543946, pvalue=0.0019274025949016982, df=50081.0)\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 07 May 2025   Prob (F-statistic):            0.00193\nTime:                        12:51:15   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nI performed a t-test to compare the donation rates between individuals who received a matching donation offer and those who did not. The test yielded a statistically significant result (p = 0.0019), indicating that those who received the match offer were more likely to donate. While the absolute difference in rates is small (from ~1.8% to ~2.2%), the result is highly unlikely to have occurred by chance. This suggests that even modest behavioral nudges — such as the promise of a matched gift — can influence charitable giving. This supports the idea presented in Table 2a of Karlan and List (2007), where matched donations are shown to increase response rates.\n\n\n\n\n\n\nProbit Model Interpretation\n\n\n\nI used a probit regression to model the likelihood that a person made a charitable donation as a function of whether they were offered a matching donation. The coefficient on the treatment variable was positive and statistically significant (z = 3.11, p = 0.002), suggesting that individuals who received the matching offer were more likely to donate.\n\n\n\nimport statsmodels.api as sm\n\n# Prepare data for probit\nX = sm.add_constant(df[\"treatment\"])  # Add intercept\ny = df[\"gave\"]\n\n# Fit probit model\nprobit_model = sm.Probit(y, X).fit()\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 07 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        12:51:15   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\n\n\n\n\n\nWhy Use a Probit Model?\n\n\n\nProbit regression is more appropriate for binary outcomes like donation (gave = 0/1) than OLS, because it models probabilities between 0 and 1 using the cumulative normal distribution. It’s a standard choice in applied microeconomics and matches the method used by Karlan & List (2007).\n\n\nAlthough the raw coefficient from a probit model isn’t directly interpretable as a probability, the significant result aligns with earlier t-tests and linear regression findings. This confirms that the treatment has a measurable effect on donation behavior. In essence, people respond to matching offers — even small nudges like this can meaningfully influence giving.\nI also took the liberty of calculating how much more likely someone is to donate because of the treatment, in percentage terms.\n\n# Get marginal effects\nmarginal_effects = probit_model.get_margeff()\nprint(marginal_effects.summary())\n\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nBeing offered a matching donation increases the probability of giving by approximately 0.43 percentage points (e.g., from 1.8% to ~2.2%). While small, this is statistically significant and meaningful in a high-volume fundraising context.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n\n\n\n\nMatch Ratio Interpretation (T-test + Regression)\n\n\n\nI conducted a series of t-tests to examine whether higher match ratios (2:1 and 3:1) led to higher donation rates compared to a 1:1 match. None of the comparisons were statistically significant. This aligns with the findings of Karlan and List (2007), who similarly reported that while matched donations increased giving compared to no match, larger match ratios did not consistently lead to higher response rates. This suggests that the presence of a match, rather than its magnitude, may be the primary behavioral driver.\n\n\n\nfrom scipy.stats import ttest_ind\n\n# 1:1 vs 2:1\ngave_1to1 = df[df[\"ratio\"] == 1][\"gave\"]\ngave_2to1 = df[df[\"ratio\"] == 2][\"gave\"]\nttest_2vs1 = ttest_ind(gave_2to1, gave_1to1)\nprint(\"2:1 vs 1:1 match rate:\", ttest_2vs1)\n\n# 2:1 vs 3:1\ngave_3to1 = df[df[\"ratio\"] == 3][\"gave\"]\nttest_3vs2 = ttest_ind(gave_3to1, gave_2to1)\nprint(\"3:1 vs 2:1 match rate:\", ttest_3vs2)\n\n# (Optional) 1:1 vs 3:1\nttest_3vs1 = ttest_ind(gave_3to1, gave_1to1)\nprint(\"3:1 vs 1:1 match rate:\", ttest_3vs1)\n\n2:1 vs 1:1 match rate: TtestResult(statistic=0.96504713432247, pvalue=0.33453168549723933, df=22265.0)\n3:1 vs 2:1 match rate: TtestResult(statistic=0.05011583793874515, pvalue=0.9600305283739325, df=22261.0)\n3:1 vs 1:1 match rate: TtestResult(statistic=1.0150255853798622, pvalue=0.3101046637086672, df=22260.0)\n\n\n\n\n\n\n\n\nWhy Regression Might Reveal More\n\n\n\nThe regression model pools variance and uses all three match groups in one estimation, which can provide slightly more power than isolated pairwise t-tests. It also allows you to assess the pattern of effects together, rather than in fragments.\n\n\n\n# Create ratio indicators\ndf[\"ratio1\"] = (df[\"ratio\"] == 1).astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == 2).astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == 3).astype(int)\n\n# Regression using manual indicators (that I just created)\nmanual_reg = smf.ols(\"gave ~ ratio2 + ratio3\", data=df).fit()\nprint(manual_reg.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     4.117\nDate:                Wed, 07 May 2025   Prob (F-statistic):             0.0163\nTime:                        12:51:15   Log-Likelihood:                 26629.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.323e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0190      0.001     22.306      0.000       0.017       0.021\nratio2         0.0036      0.002      2.269      0.023       0.000       0.007\nratio3         0.0037      0.002      2.332      0.020       0.001       0.007\n==============================================================================\nOmnibus:                    59815.856   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317637.927\nSkew:                           6.741   Prob(JB):                         0.00\nKurtosis:                      46.443   Cond. No.                         3.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIn this regression, I examined whether the size of the match ratio influenced the likelihood of donating. The model uses the 1:1 match group as the baseline. Both the 2:1 and 3:1 match groups showed small but statistically significant increases in donation rates, around +0.36 to +0.37 percentage points. This contrasts slightly with the t-tests, which did not find significant differences — likely because the regression model uses more degrees of freedom and pools variance more efficiently.\nThe results suggest that larger match ratios may have had a small positive impact, though the effect size is modest. This provides slightly more evidence than Karlan and List (2007), who found no consistent effects. However, even in this case, the behavioral change is relatively small, supporting the idea that the presence of a match may matter more than its magnitude.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nBelow is the T-Test:\n\nfrom scipy.stats import ttest_ind\n\n# T-test comparing average donation amount (including 0s)\nt_amount = ttest_ind(df[df[\"treatment\"] == 1][\"amount\"],\n                     df[df[\"treatment\"] == 0][\"amount\"])\nprint(\"T-test on donation amount:\")\nprint(t_amount)\n\nT-test on donation amount:\nTtestResult(statistic=1.8605020225753781, pvalue=0.06282038947470686, df=50081.0)\n\n\nThe results of this indicate:\nNow I am running a bivariate linear regression of the donation amount on the treatment status. It shows:\n\nimport statsmodels.formula.api as smf\n\n# OLS regression of donation amount on treatment\namount_reg = smf.ols(\"amount ~ treatment\", data=df).fit()\nprint(amount_reg.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 07 May 2025   Prob (F-statistic):             0.0628\nTime:                        12:51:15   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\nOverall Donation Amount Interpretation\n\n\n\nI analyzed whether people in the treatment group gave more money, on average, than those in the control group. Both a t-test and a linear regression yielded nearly identical results: treatment group participants donated about $0.15 more, but the difference was not statistically significant (p ≈ 0.063). This suggests that while there may be a small increase in donation amount due to the treatment, the effect is weak and uncertain when including non-donors in the analysis.\n\n\n\nimport statsmodels.formula.api as smf\n\n# Filter to only people who donated\ndonors = df[df[\"amount\"] &gt; 0]\n\n# Run regression on amount given, conditional on donating\ndonation_reg = smf.ols(\"amount ~ treatment\", data=donors).fit()\nprint(donation_reg.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 07 May 2025   Prob (F-statistic):              0.561\nTime:                        12:51:15   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\nConditional Donation Amount Interpretation\n\n\n\nI restricted the sample to only those who donated and regressed the donation amount on treatment status. The results showed that donors in the treatment group gave, on average, $1.67 less than those in the control group. However, this difference was not statistically significant (p = 0.561). This suggests that while the match offer may have influenced the decision to donate, it did not affect how much was given among those who did donate.\n\n\n\n\n\n\n\n\nCausality Caveat\n\n\n\nThis regression is not strictly causal, because it conditions on having donated — which is a post-treatment outcome. That means you’re selecting a subset of people whose outcome (donating) may itself have been influenced by the treatment. This could introduce selection bias, so while the analysis is informative, it should be interpreted descriptively rather than causally.\n\n\n\n\n\n\n\n\nHistogram Interpretation\n\n\n\nI created histograms of donation amounts among donors in both the treatment and control groups. Both groups show similar distributions, with most donations clustering around $25 to $75. The red line shows the mean donation, which is slightly higher in the control group. These visuals reinforce the regression results — while the treatment may have influenced donation likelihood, it did not lead to higher amounts among those who gave.\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Filter to donors only\ndonors = df[df[\"amount\"] &gt; 0]\n\n# --- Histogram for Treatment Group ---\nsns.histplot(data=donors[donors[\"treatment\"] == 1], x=\"amount\", bins=30, kde=False)\nplt.axvline(donors[donors[\"treatment\"] == 1][\"amount\"].mean(), color='red', linestyle='--', label=\"Mean\")\nplt.title(\"Donation Amounts – Treatment Group\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()\n\n# --- Histogram for Control Group ---\nsns.histplot(data=donors[donors[\"treatment\"] == 0], x=\"amount\", bins=30, kde=False)\nplt.axvline(donors[donors[\"treatment\"] == 0][\"amount\"].mean(), color='red', linestyle='--', label=\"Mean\")\nplt.title(\"Donation Amounts – Control Group\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "project_assignments/HW1/hw1_questions.html#simulation-experiment",
    "href": "project_assignments/HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\nTo illustrate the Law of Large Numbers, I simulated differences in donation rates between treatment and control groups and plotted the cumulative average over 10,000 draws. While early sample differences fluctuated substantially, the average steadily approached the true treatment effect of 0.004. This demonstrates how, with large enough samples, the sample average converges to the population mean — providing reassurance about the reliability of randomized experimental estimates.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Simulate donation outcomes (1 = gave, 0 = didn’t give)\ncontrol_draws = np.random.binomial(1, 0.018, 10000)\ntreatment_draws = np.random.binomial(1, 0.022, 10000)\n\n# Calculate the difference in each trial\ndiffs = treatment_draws - control_draws\n\n# Cumulative average of differences\ncum_avg = np.cumsum(diffs) / (np.arange(1, len(diffs)+1))\n\n# Plot the cumulative average\nplt.plot(cum_avg, color='orange')\nplt.axhline(0.004, color='red', linestyle='--', label=\"True Effect (0.004)\")\nplt.title(\"Cumulative Average of Treatment - Control Differences\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nI simulated 1000 samples for four different sizes (n = 50, 200, 500, 1000) and plotted the sampling distribution of the mean difference in donation rates. As expected, with increasing sample size, the distributions became more symmetric and concentrated around the true treatment effect. This visualizes the Central Limit Theorem — regardless of the population distribution, the sampling distribution of the mean becomes approximately normal as the sample size grows.\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nsample_sizes = [50, 200, 500, 1000]\nnp.random.seed(42)\n\nfor n in sample_sizes:\n    sim_means = []\n    for _ in range(1000):\n        control_sample = np.random.binomial(1, 0.018, n)\n        treatment_sample = np.random.binomial(1, 0.022, n)\n        sim_means.append(np.mean(treatment_sample - control_sample))\n\n    # Plot histogram of sampling distribution\n    plt.figure()\n    sns.histplot(sim_means, bins=30)\n    plt.title(f\"Sampling Distribution of Mean Difference (n = {n})\")\n    plt.xlabel(\"Mean Difference\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(True)\n    plt.show()"
  },
  {
    "objectID": "project_assignments/HW2/hw2_questions.html",
    "href": "project_assignments/HW2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport warnings\nimport pandas as pd\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\nwarnings.simplefilter(action='ignore', category=RuntimeWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\nfrom scipy.optimize import OptimizeWarning\nwarnings.simplefilter(action='ignore', category=OptimizeWarning)\n\n\n# Load necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load Blueprinty data\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Display first few rows\nblueprinty.head()\n\n# Transformations of Age referenced later in the process\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_sq_std\"] = blueprinty[\"age_std\"] ** 2\n\n\n# Summary statistics\nblueprinty.describe(include='all')\n\n# Data types and missing values\nblueprinty.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \n 4   age_std     1500 non-null   float64\n 5   age_sq_std  1500 non-null   float64\ndtypes: float64(3), int64(2), object(1)\nmemory usage: 70.4+ KB\n\n\n\n# Group data by customer status and calculate mean number of patents\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\"Mean number of patents by customer status:\\n\", mean_patents)\n\n# Plot histograms of number of patents for each group\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer\")\nplt.show()\n\nMean number of patents by customer status:\n iscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram Interpretation\n\n\n\nWe observe that customers tend to have a higher number of patents on average, and their distribution is right-skewed compared to non-customers. This justifies including iscustomer as a predictor in our Poisson regression model.\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Compare average age by customer status\nage_by_customer = blueprinty.groupby(\"iscustomer\")[\"age\"].describe()\nprint(\"Age summary by customer status:\\n\", age_by_customer)\n\n# Plot age distributions\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer\")\nplt.show()\n\nAge summary by customer status:\n              count       mean       std   min   25%   50%    75%   max\niscustomer                                                            \n0           1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\n1            481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge Distribution Interpretation\n\n\n\nThe age distribution reveals that customers (orange) tend to skew slightly older than non-customers (blue).\nWhile both groups peak around the mid- to late-20s, customers show a broader spread into their 30s and 40s,\nsuggesting that older individuals may be more likely to become customers.\n\n\n\n# Region counts by customer status\nregion_counts = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"])\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0)\nregion_props.plot(kind=\"bar\", stacked=True)\nplt.title(\"Proportion of Customers by Region\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Customer\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionality Distribution Interpretation\n\n\n\nRegionally, the Northeast stands out: it has the lowest proportion of customers,\nwith more than half of individuals in that region being non-customers.\nIn contrast, the Midwest, Northwest, South, and Southwest all show a strong majority of customers.\nThis geographic pattern implies that regional targeting or market presence might be influencing customer conversion.\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet ( Y_1, Y_2, , Y_n () ). The probability mass function is:\n[ f(Y_i ) = ]\nThen the likelihood function for a sample of size ( n ) is:\n[ (Y_1, , Y_n) = _{i=1}^{n} ]\nTaking the natural logarithm to get the log-likelihood:\n[ () = _{i=1}^{n} ( -+ Y_i - Y_i! ) ]\nThis is the log-likelihood expression we will use to estimate ( ) in our simple Poisson model.\n\n# Define the log-likelihood function for the Poisson model\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lamb, Y):\n    if lamb &lt;= 0:\n        return -np.inf\n    return np.sum(-lamb + Y * np.log(lamb) - gammaln(Y + 1))\n\n\nimport matplotlib.pyplot as plt\n\n# Extract observed Y values (number of patents)\nY = blueprinty[\"patents\"].values\n\n# Create a range of lambda values\nlambda_vals = np.linspace(0.1, 10, 200)  # Avoid 0 to prevent log(0)\n\n# Calculate log-likelihood for each lambda\nloglik_vals = [poisson_loglikelihood(lam, Y) for lam in lambda_vals]\n\n# Plot\nplt.plot(lambda_vals, loglik_vals)\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Likelihood Curve Interpretation\n\n\n\nThe log-likelihood curve reaches a clear peak, suggesting the maximum likelihood estimate (MLE) of ( ) lies around that peak.\nThis visual check helps confirm the function is well-behaved and the model is appropriate for estimating a central rate parameter from count data.\n\n\nWe start with the log-likelihood for ( n ) i.i.d. observations from a Poisson distribution:\n[ () = _{i=1}^{n} ( -+ Y_i - Y_i! ) ]\nTo find the MLE of ( ), we take the first derivative with respect to ( ):\n[ = {i=1}^{n} ( -1 + ) = -n + {i=1}^{n} Y_i ]\nSet the derivative equal to zero and solve for ( ):\n[ -n + {i=1}^{n} Y_i = 0 {i=1}^{n} Y_i = n = _{i=1}^{n} Y_i = {Y} ]\nThus, the MLE of ( ) is simply the sample mean ( {Y} ).\nThis makes intuitive sense, since the Poisson distribution has both its mean and variance equal to ( ).\n\nfrom scipy.optimize import minimize_scalar\n\n# Use the same data (patent counts)\nY = blueprinty[\"patents\"].values\n\n# Negative log-likelihood for minimization\ndef neg_loglik(lamb):\n    return -poisson_loglikelihood(lamb, Y)\n\n# Minimize over a reasonable range\nresult = minimize_scalar(neg_loglik, bounds=(0.1, 10), method='bounded')\n\n# Extract MLE\nlambda_mle = result.x\nprint(f\"MLE of lambda (numerical optimization): {lambda_mle:.4f}\")\n\n# Compare to sample mean\nsample_mean = np.mean(Y)\nprint(f\"Sample mean of Y: {sample_mean:.4f}\")\n\nMLE of lambda (numerical optimization): 3.6847\nSample mean of Y: 3.6847\n\n\n\n\n\n\n\n\nResults from Optimization\n\n\n\nUsing numerical optimization, the estimated MLE of ( ) is approximately {lambda_mle:.4f},\nwhich aligns closely with the sample mean ( {Y} = {sample_mean:.4f} ).\nThis matches our earlier mathematical derivation, confirming that the Poisson MLE for ( ) is the mean of the observed data.\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\n\nUpdated Log-Likelihood Function\n\n\n\n\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglik(beta, Y, X):\n    # Ensure all inputs are NumPy arrays with correct dtype\n    beta = np.asarray(beta, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    X = np.asarray(X, dtype=float)\n\n    # Compute linear predictor and lambda\n    lin_pred = X @ beta\n    lamb = np.exp(lin_pred)\n\n    # Return log-likelihood\n    return np.sum(-lamb + Y * np.log(lamb) - gammaln(Y + 1))\n\n\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\n\n\n\n\nBuild the Design Matrix X\n\n\n\n\n# Create region dummies (drop one category for baseline)\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\n# Construct the design matrix using standardized variables\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age_std\", \"age_sq_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\n# Convert to NumPy\nX_np = X.to_numpy(dtype=float)\nY = blueprinty[\"patents\"].to_numpy(dtype=float)\n\n\n\n\n\n\n\n\n\nEstimate 𝛽\n\n\n\n\nfrom scipy.optimize import minimize\n\n# Negative log-likelihood for minimization\ndef neg_loglik(beta, Y, X):\n    return -poisson_regression_loglik(beta, Y, X)\n\n# Initial guess: zeros\ninit_beta = np.zeros(X_np.shape[1])\n\n# Perform optimization with Hessian output\nresult = minimize(\n    fun=neg_loglik,\n    x0=init_beta,\n    args=(Y, X_np),\n    method=\"BFGS\",\n    options={\"disp\": True}\n)\n\n# Extract estimates and Hessian\nbeta_hat = result.x\nhessian = result.hess_inv  # Approximate inverse Hessian from BFGS\n\n         Current function value: 3258.072145\n         Iterations: 18\n         Function evaluations: 252\n         Gradient evaluations: 28\n\n\n\n\n\n\n\n\nExtracting Coefficients\n\n\n\n\n# Extract coefficients and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv  # BFGS approximation to inverse Hessian\nse = np.sqrt(np.diag(hessian_inv))\n\n# Create summary table\nsummary_df = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": se\n})\nsummary_df[\"Coefficient\"] = summary_df[\"Coefficient\"].round(4)\nsummary_df[\"Std. Error\"] = se.round(4)\n\n# Reorder columns\nsummary_df = summary_df[[\"Variable\", \"Coefficient\", \"Std. Error\"]]\n\nsummary_df\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.3447\n0.0344\n\n\n1\nage_std\n-0.0577\n0.0149\n\n\n2\nage_sq_std\n-0.1558\n0.0172\n\n\n3\niscustomer\n0.2076\n0.0314\n\n\n4\nNortheast\n0.0292\n0.0382\n\n\n5\nNorthwest\n-0.0176\n0.0231\n\n\n6\nSouth\n0.0566\n0.0452\n\n\n7\nSouthwest\n0.0506\n0.0417\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Coefficients\n\n\n\nThe coefficients represent the estimated effect of each variable on the log expected patent count.\n\nBeing a customer of Blueprinty is associated with a significant increase in patent counts (β = 0.208, SE = 0.031).\nAge has a small negative effect, and the negative age-squared term suggests a concave relationship — i.e., patenting peaks in mid-career.\nRegional effects are relatively minor, with South and Southwest showing small positive deviations.\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit Poisson regression using statsmodels GLM\nglm_model = sm.GLM(Y, X_np, family=sm.families.Poisson()).fit()\n\n# Display summary\nprint(glm_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 12 May 2025   Deviance:                       2143.3\nTime:                        09:54:11   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.3447      0.038     35.059      0.000       1.270       1.420\nx1            -0.0577      0.015     -3.843      0.000      -0.087      -0.028\nx2            -0.1558      0.014    -11.513      0.000      -0.182      -0.129\nx3             0.2076      0.031      6.719      0.000       0.147       0.268\nx4             0.0292      0.044      0.669      0.504      -0.056       0.115\nx5            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx6             0.0566      0.053      1.074      0.283      -0.047       0.160\nx7             0.0506      0.047      1.072      0.284      -0.042       0.143\n==============================================================================\n\n\n\n\n\n\n\n\nModel Validation with statsmodels\n\n\n\nTo confirm the accuracy of our custom maximum likelihood estimation (MLE), we refit the same Poisson regression using Python’s built-in statsmodels.GLM() function.\nThe resulting coefficients and standard errors were nearly identical to our hand-coded implementation, validating both the numerical optimization and our understanding of Poisson regression mechanics.\n\n\n\n# Step 1: Copy design matrix and modify iscustomer column\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Step 2: Convert to NumPy\nX0_np = X_0.to_numpy(dtype=float)\nX1_np = X_1.to_numpy(dtype=float)\n\n# Step 3: Predicted lambda values using your fitted beta\nlambda_0 = np.exp(X0_np @ beta_hat)\nlambda_1 = np.exp(X1_np @ beta_hat)\n\n# Step 4: Difference in predicted patent counts\ndelta = lambda_1 - lambda_0\n\n# Step 5: Average effect of being a customer\naverage_effect = np.mean(delta)\nprint(f\"Average increase in predicted patents from being a customer: {average_effect:.4f}\")\n\nAverage increase in predicted patents from being a customer: 0.7928\n\n\n\n\n\n\n\n\nFinal Analysis\n\n\n\nTo assess the effect of Blueprinty’s software on patent success, we simulated expected patent counts for all firms under two scenarios:\none where no firms were customers, and another where all firms were.\nThe analysis reveals that, on average, being a Blueprinty customer increases expected patent output by approximately 0.79 patents per firm.\nThis suggests a meaningful positive effect of the software on innovation activity.\n\n\n\n\n\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n# Load Airbnb data\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# Quick look at the data\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# Summary stats\nairbnb.describe(include=\"all\")\n\n# Check missing values\nairbnb.isna().sum()\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\n\n\n\n\n\n\nFeature Engineering and Design Matrix\n\n\n\n\n# Drop only rows with missing values in relevant model variables\nmodel_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\", \"room_type\"\n]\n\nairbnb_clean = airbnb.dropna(subset=model_vars)\nairbnb_clean = airbnb_clean.copy()\n\n# Binary encoding for instant_bookable\nairbnb_clean[\"instant_bookable_bin\"] = (airbnb_clean[\"instant_bookable\"] == \"t\").astype(int)\n\n# Standardize price and days\nairbnb_clean[\"price_std\"] = (airbnb_clean[\"price\"] - airbnb_clean[\"price\"].mean()) / airbnb_clean[\"price\"].std()\nairbnb_clean[\"days_std\"] = (airbnb_clean[\"days\"] - airbnb_clean[\"days\"].mean()) / airbnb_clean[\"days\"].std()\n\n# Room type dummies (drop one for baseline)\nroom_dummies = pd.get_dummies(airbnb_clean[\"room_type\"], drop_first=True)\n\n# Build X matrix\nX_airbnb = pd.concat([\n    pd.Series(1, index=airbnb_clean.index, name=\"intercept\"),\n    airbnb_clean[[\n        \"bathrooms\", \"bedrooms\", \"review_scores_cleanliness\",\n        \"review_scores_location\", \"review_scores_value\",\n        \"instant_bookable_bin\", \"price_std\", \"days_std\"\n    ]],\n    room_dummies\n], axis=1)\n\n# Convert to NumPy for modeling\nX_airbnb_np = X_airbnb.to_numpy(dtype=float)\nY_airbnb = airbnb_clean[\"number_of_reviews\"].to_numpy(dtype=float)\n\n\n\n\n\n\n\n\n\nImplementing the Poission Regression\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit model\nglm_airbnb = sm.GLM(Y_airbnb, X_airbnb_np, family=sm.families.Poisson()).fit()\n\n# View summary\nprint(glm_airbnb.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30149\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2418e+05\nDate:                Mon, 12 May 2025   Deviance:                   9.2689e+05\nTime:                        09:54:12   Pearson chi2:                 1.37e+06\nNo. Iterations:                    10   Pseudo R-squ. (CS):             0.6840\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.5533      0.016    219.754      0.000       3.522       3.585\nx1            -0.1177      0.004    -31.394      0.000      -0.125      -0.110\nx2             0.0741      0.002     37.197      0.000       0.070       0.078\nx3             0.1131      0.001     75.611      0.000       0.110       0.116\nx4            -0.0769      0.002    -47.796      0.000      -0.080      -0.074\nx5            -0.0911      0.002    -50.490      0.000      -0.095      -0.088\nx6             0.3459      0.003    119.666      0.000       0.340       0.352\nx7            -0.0034      0.002     -2.151      0.031      -0.006      -0.000\nx8             0.0635      0.000    129.755      0.000       0.063       0.064\nx9            -0.0105      0.003     -3.847      0.000      -0.016      -0.005\nx10           -0.2463      0.009    -28.578      0.000      -0.263      -0.229\n==============================================================================\n\n\n\n\n\n\n\n\n\n\nAirBNB Reviews Interpretation\n\n\n\nWe modeled the number of reviews (as a proxy for bookings) using Poisson regression. Key findings include:\n\nInstant bookable listings are significantly more likely to get reviews — suggesting ease of booking matters to users.\nLarger listings (more bedrooms) and cleanliness scores are also strong positive predictors.\nShared rooms have much fewer bookings than entire homes, and Private rooms also see reduced volume.\nHigher prices slightly reduce bookings, consistent with price sensitivity.\n“Value” and “location” scores were surprisingly negative, possibly reflecting underlying price or geography-related confounders.\n\nOverall, the model helps identify which listing attributes are associated with higher demand in NYC’s Airbnb market."
  },
  {
    "objectID": "project_assignments/HW2/hw2_questions.html#blueprinty-case-study",
    "href": "project_assignments/HW2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport warnings\nimport pandas as pd\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\nwarnings.simplefilter(action='ignore', category=RuntimeWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\nfrom scipy.optimize import OptimizeWarning\nwarnings.simplefilter(action='ignore', category=OptimizeWarning)\n\n\n# Load necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load Blueprinty data\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# Display first few rows\nblueprinty.head()\n\n# Transformations of Age referenced later in the process\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_sq_std\"] = blueprinty[\"age_std\"] ** 2\n\n\n# Summary statistics\nblueprinty.describe(include='all')\n\n# Data types and missing values\nblueprinty.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   patents     1500 non-null   int64  \n 1   region      1500 non-null   object \n 2   age         1500 non-null   float64\n 3   iscustomer  1500 non-null   int64  \n 4   age_std     1500 non-null   float64\n 5   age_sq_std  1500 non-null   float64\ndtypes: float64(3), int64(2), object(1)\nmemory usage: 70.4+ KB\n\n\n\n# Group data by customer status and calculate mean number of patents\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\"Mean number of patents by customer status:\\n\", mean_patents)\n\n# Plot histograms of number of patents for each group\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer\")\nplt.show()\n\nMean number of patents by customer status:\n iscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram Interpretation\n\n\n\nWe observe that customers tend to have a higher number of patents on average, and their distribution is right-skewed compared to non-customers. This justifies including iscustomer as a predictor in our Poisson regression model.\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Compare average age by customer status\nage_by_customer = blueprinty.groupby(\"iscustomer\")[\"age\"].describe()\nprint(\"Age summary by customer status:\\n\", age_by_customer)\n\n# Plot age distributions\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer\")\nplt.show()\n\nAge summary by customer status:\n              count       mean       std   min   25%   50%    75%   max\niscustomer                                                            \n0           1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\n1            481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge Distribution Interpretation\n\n\n\nThe age distribution reveals that customers (orange) tend to skew slightly older than non-customers (blue).\nWhile both groups peak around the mid- to late-20s, customers show a broader spread into their 30s and 40s,\nsuggesting that older individuals may be more likely to become customers.\n\n\n\n# Region counts by customer status\nregion_counts = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"])\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0)\nregion_props.plot(kind=\"bar\", stacked=True)\nplt.title(\"Proportion of Customers by Region\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Customer\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionality Distribution Interpretation\n\n\n\nRegionally, the Northeast stands out: it has the lowest proportion of customers,\nwith more than half of individuals in that region being non-customers.\nIn contrast, the Midwest, Northwest, South, and Southwest all show a strong majority of customers.\nThis geographic pattern implies that regional targeting or market presence might be influencing customer conversion.\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet ( Y_1, Y_2, , Y_n () ). The probability mass function is:\n[ f(Y_i ) = ]\nThen the likelihood function for a sample of size ( n ) is:\n[ (Y_1, , Y_n) = _{i=1}^{n} ]\nTaking the natural logarithm to get the log-likelihood:\n[ () = _{i=1}^{n} ( -+ Y_i - Y_i! ) ]\nThis is the log-likelihood expression we will use to estimate ( ) in our simple Poisson model.\n\n# Define the log-likelihood function for the Poisson model\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lamb, Y):\n    if lamb &lt;= 0:\n        return -np.inf\n    return np.sum(-lamb + Y * np.log(lamb) - gammaln(Y + 1))\n\n\nimport matplotlib.pyplot as plt\n\n# Extract observed Y values (number of patents)\nY = blueprinty[\"patents\"].values\n\n# Create a range of lambda values\nlambda_vals = np.linspace(0.1, 10, 200)  # Avoid 0 to prevent log(0)\n\n# Calculate log-likelihood for each lambda\nloglik_vals = [poisson_loglikelihood(lam, Y) for lam in lambda_vals]\n\n# Plot\nplt.plot(lambda_vals, loglik_vals)\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Likelihood Curve Interpretation\n\n\n\nThe log-likelihood curve reaches a clear peak, suggesting the maximum likelihood estimate (MLE) of ( ) lies around that peak.\nThis visual check helps confirm the function is well-behaved and the model is appropriate for estimating a central rate parameter from count data.\n\n\nWe start with the log-likelihood for ( n ) i.i.d. observations from a Poisson distribution:\n[ () = _{i=1}^{n} ( -+ Y_i - Y_i! ) ]\nTo find the MLE of ( ), we take the first derivative with respect to ( ):\n[ = {i=1}^{n} ( -1 + ) = -n + {i=1}^{n} Y_i ]\nSet the derivative equal to zero and solve for ( ):\n[ -n + {i=1}^{n} Y_i = 0 {i=1}^{n} Y_i = n = _{i=1}^{n} Y_i = {Y} ]\nThus, the MLE of ( ) is simply the sample mean ( {Y} ).\nThis makes intuitive sense, since the Poisson distribution has both its mean and variance equal to ( ).\n\nfrom scipy.optimize import minimize_scalar\n\n# Use the same data (patent counts)\nY = blueprinty[\"patents\"].values\n\n# Negative log-likelihood for minimization\ndef neg_loglik(lamb):\n    return -poisson_loglikelihood(lamb, Y)\n\n# Minimize over a reasonable range\nresult = minimize_scalar(neg_loglik, bounds=(0.1, 10), method='bounded')\n\n# Extract MLE\nlambda_mle = result.x\nprint(f\"MLE of lambda (numerical optimization): {lambda_mle:.4f}\")\n\n# Compare to sample mean\nsample_mean = np.mean(Y)\nprint(f\"Sample mean of Y: {sample_mean:.4f}\")\n\nMLE of lambda (numerical optimization): 3.6847\nSample mean of Y: 3.6847\n\n\n\n\n\n\n\n\nResults from Optimization\n\n\n\nUsing numerical optimization, the estimated MLE of ( ) is approximately {lambda_mle:.4f},\nwhich aligns closely with the sample mean ( {Y} = {sample_mean:.4f} ).\nThis matches our earlier mathematical derivation, confirming that the Poisson MLE for ( ) is the mean of the observed data.\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\n\nUpdated Log-Likelihood Function\n\n\n\n\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglik(beta, Y, X):\n    # Ensure all inputs are NumPy arrays with correct dtype\n    beta = np.asarray(beta, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    X = np.asarray(X, dtype=float)\n\n    # Compute linear predictor and lambda\n    lin_pred = X @ beta\n    lamb = np.exp(lin_pred)\n\n    # Return log-likelihood\n    return np.sum(-lamb + Y * np.log(lamb) - gammaln(Y + 1))\n\n\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\n\n\n\n\nBuild the Design Matrix X\n\n\n\n\n# Create region dummies (drop one category for baseline)\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\n# Construct the design matrix using standardized variables\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age_std\", \"age_sq_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\n# Convert to NumPy\nX_np = X.to_numpy(dtype=float)\nY = blueprinty[\"patents\"].to_numpy(dtype=float)\n\n\n\n\n\n\n\n\n\nEstimate 𝛽\n\n\n\n\nfrom scipy.optimize import minimize\n\n# Negative log-likelihood for minimization\ndef neg_loglik(beta, Y, X):\n    return -poisson_regression_loglik(beta, Y, X)\n\n# Initial guess: zeros\ninit_beta = np.zeros(X_np.shape[1])\n\n# Perform optimization with Hessian output\nresult = minimize(\n    fun=neg_loglik,\n    x0=init_beta,\n    args=(Y, X_np),\n    method=\"BFGS\",\n    options={\"disp\": True}\n)\n\n# Extract estimates and Hessian\nbeta_hat = result.x\nhessian = result.hess_inv  # Approximate inverse Hessian from BFGS\n\n         Current function value: 3258.072145\n         Iterations: 18\n         Function evaluations: 252\n         Gradient evaluations: 28\n\n\n\n\n\n\n\n\nExtracting Coefficients\n\n\n\n\n# Extract coefficients and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv  # BFGS approximation to inverse Hessian\nse = np.sqrt(np.diag(hessian_inv))\n\n# Create summary table\nsummary_df = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": se\n})\nsummary_df[\"Coefficient\"] = summary_df[\"Coefficient\"].round(4)\nsummary_df[\"Std. Error\"] = se.round(4)\n\n# Reorder columns\nsummary_df = summary_df[[\"Variable\", \"Coefficient\", \"Std. Error\"]]\n\nsummary_df\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.3447\n0.0344\n\n\n1\nage_std\n-0.0577\n0.0149\n\n\n2\nage_sq_std\n-0.1558\n0.0172\n\n\n3\niscustomer\n0.2076\n0.0314\n\n\n4\nNortheast\n0.0292\n0.0382\n\n\n5\nNorthwest\n-0.0176\n0.0231\n\n\n6\nSouth\n0.0566\n0.0452\n\n\n7\nSouthwest\n0.0506\n0.0417\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Coefficients\n\n\n\nThe coefficients represent the estimated effect of each variable on the log expected patent count.\n\nBeing a customer of Blueprinty is associated with a significant increase in patent counts (β = 0.208, SE = 0.031).\nAge has a small negative effect, and the negative age-squared term suggests a concave relationship — i.e., patenting peaks in mid-career.\nRegional effects are relatively minor, with South and Southwest showing small positive deviations.\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit Poisson regression using statsmodels GLM\nglm_model = sm.GLM(Y, X_np, family=sm.families.Poisson()).fit()\n\n# Display summary\nprint(glm_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 12 May 2025   Deviance:                       2143.3\nTime:                        09:54:11   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.3447      0.038     35.059      0.000       1.270       1.420\nx1            -0.0577      0.015     -3.843      0.000      -0.087      -0.028\nx2            -0.1558      0.014    -11.513      0.000      -0.182      -0.129\nx3             0.2076      0.031      6.719      0.000       0.147       0.268\nx4             0.0292      0.044      0.669      0.504      -0.056       0.115\nx5            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx6             0.0566      0.053      1.074      0.283      -0.047       0.160\nx7             0.0506      0.047      1.072      0.284      -0.042       0.143\n==============================================================================\n\n\n\n\n\n\n\n\nModel Validation with statsmodels\n\n\n\nTo confirm the accuracy of our custom maximum likelihood estimation (MLE), we refit the same Poisson regression using Python’s built-in statsmodels.GLM() function.\nThe resulting coefficients and standard errors were nearly identical to our hand-coded implementation, validating both the numerical optimization and our understanding of Poisson regression mechanics.\n\n\n\n# Step 1: Copy design matrix and modify iscustomer column\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Step 2: Convert to NumPy\nX0_np = X_0.to_numpy(dtype=float)\nX1_np = X_1.to_numpy(dtype=float)\n\n# Step 3: Predicted lambda values using your fitted beta\nlambda_0 = np.exp(X0_np @ beta_hat)\nlambda_1 = np.exp(X1_np @ beta_hat)\n\n# Step 4: Difference in predicted patent counts\ndelta = lambda_1 - lambda_0\n\n# Step 5: Average effect of being a customer\naverage_effect = np.mean(delta)\nprint(f\"Average increase in predicted patents from being a customer: {average_effect:.4f}\")\n\nAverage increase in predicted patents from being a customer: 0.7928\n\n\n\n\n\n\n\n\nFinal Analysis\n\n\n\nTo assess the effect of Blueprinty’s software on patent success, we simulated expected patent counts for all firms under two scenarios:\none where no firms were customers, and another where all firms were.\nThe analysis reveals that, on average, being a Blueprinty customer increases expected patent output by approximately 0.79 patents per firm.\nThis suggests a meaningful positive effect of the software on innovation activity.\n\n\n\n\n\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n# Load Airbnb data\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# Quick look at the data\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# Summary stats\nairbnb.describe(include=\"all\")\n\n# Check missing values\nairbnb.isna().sum()\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\n\n\n\n\n\n\nFeature Engineering and Design Matrix\n\n\n\n\n# Drop only rows with missing values in relevant model variables\nmodel_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\", \"room_type\"\n]\n\nairbnb_clean = airbnb.dropna(subset=model_vars)\nairbnb_clean = airbnb_clean.copy()\n\n# Binary encoding for instant_bookable\nairbnb_clean[\"instant_bookable_bin\"] = (airbnb_clean[\"instant_bookable\"] == \"t\").astype(int)\n\n# Standardize price and days\nairbnb_clean[\"price_std\"] = (airbnb_clean[\"price\"] - airbnb_clean[\"price\"].mean()) / airbnb_clean[\"price\"].std()\nairbnb_clean[\"days_std\"] = (airbnb_clean[\"days\"] - airbnb_clean[\"days\"].mean()) / airbnb_clean[\"days\"].std()\n\n# Room type dummies (drop one for baseline)\nroom_dummies = pd.get_dummies(airbnb_clean[\"room_type\"], drop_first=True)\n\n# Build X matrix\nX_airbnb = pd.concat([\n    pd.Series(1, index=airbnb_clean.index, name=\"intercept\"),\n    airbnb_clean[[\n        \"bathrooms\", \"bedrooms\", \"review_scores_cleanliness\",\n        \"review_scores_location\", \"review_scores_value\",\n        \"instant_bookable_bin\", \"price_std\", \"days_std\"\n    ]],\n    room_dummies\n], axis=1)\n\n# Convert to NumPy for modeling\nX_airbnb_np = X_airbnb.to_numpy(dtype=float)\nY_airbnb = airbnb_clean[\"number_of_reviews\"].to_numpy(dtype=float)\n\n\n\n\n\n\n\n\n\nImplementing the Poission Regression\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit model\nglm_airbnb = sm.GLM(Y_airbnb, X_airbnb_np, family=sm.families.Poisson()).fit()\n\n# View summary\nprint(glm_airbnb.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30149\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2418e+05\nDate:                Mon, 12 May 2025   Deviance:                   9.2689e+05\nTime:                        09:54:12   Pearson chi2:                 1.37e+06\nNo. Iterations:                    10   Pseudo R-squ. (CS):             0.6840\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.5533      0.016    219.754      0.000       3.522       3.585\nx1            -0.1177      0.004    -31.394      0.000      -0.125      -0.110\nx2             0.0741      0.002     37.197      0.000       0.070       0.078\nx3             0.1131      0.001     75.611      0.000       0.110       0.116\nx4            -0.0769      0.002    -47.796      0.000      -0.080      -0.074\nx5            -0.0911      0.002    -50.490      0.000      -0.095      -0.088\nx6             0.3459      0.003    119.666      0.000       0.340       0.352\nx7            -0.0034      0.002     -2.151      0.031      -0.006      -0.000\nx8             0.0635      0.000    129.755      0.000       0.063       0.064\nx9            -0.0105      0.003     -3.847      0.000      -0.016      -0.005\nx10           -0.2463      0.009    -28.578      0.000      -0.263      -0.229\n==============================================================================\n\n\n\n\n\n\n\n\n\n\nAirBNB Reviews Interpretation\n\n\n\nWe modeled the number of reviews (as a proxy for bookings) using Poisson regression. Key findings include:\n\nInstant bookable listings are significantly more likely to get reviews — suggesting ease of booking matters to users.\nLarger listings (more bedrooms) and cleanliness scores are also strong positive predictors.\nShared rooms have much fewer bookings than entire homes, and Private rooms also see reduced volume.\nHigher prices slightly reduce bookings, consistent with price sensitivity.\n“Value” and “location” scores were surprisingly negative, possibly reflecting underlying price or geography-related confounders.\n\nOverall, the model helps identify which listing attributes are associated with higher demand in NYC’s Airbnb market."
  },
  {
    "objectID": "project_assignments/HW2/hw2_questions.html#airbnb-case-study",
    "href": "project_assignments/HW2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n# Load Airbnb data\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# Quick look at the data\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# Summary stats\nairbnb.describe(include=\"all\")\n\n# Check missing values\nairbnb.isna().sum()\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\n\n\n\n\n\n\nFeature Engineering and Design Matrix\n\n\n\n\n# Drop only rows with missing values in relevant model variables\nmodel_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\", \"room_type\"\n]\n\nairbnb_clean = airbnb.dropna(subset=model_vars)\nairbnb_clean = airbnb_clean.copy()\n\n# Binary encoding for instant_bookable\nairbnb_clean[\"instant_bookable_bin\"] = (airbnb_clean[\"instant_bookable\"] == \"t\").astype(int)\n\n# Standardize price and days\nairbnb_clean[\"price_std\"] = (airbnb_clean[\"price\"] - airbnb_clean[\"price\"].mean()) / airbnb_clean[\"price\"].std()\nairbnb_clean[\"days_std\"] = (airbnb_clean[\"days\"] - airbnb_clean[\"days\"].mean()) / airbnb_clean[\"days\"].std()\n\n# Room type dummies (drop one for baseline)\nroom_dummies = pd.get_dummies(airbnb_clean[\"room_type\"], drop_first=True)\n\n# Build X matrix\nX_airbnb = pd.concat([\n    pd.Series(1, index=airbnb_clean.index, name=\"intercept\"),\n    airbnb_clean[[\n        \"bathrooms\", \"bedrooms\", \"review_scores_cleanliness\",\n        \"review_scores_location\", \"review_scores_value\",\n        \"instant_bookable_bin\", \"price_std\", \"days_std\"\n    ]],\n    room_dummies\n], axis=1)\n\n# Convert to NumPy for modeling\nX_airbnb_np = X_airbnb.to_numpy(dtype=float)\nY_airbnb = airbnb_clean[\"number_of_reviews\"].to_numpy(dtype=float)\n\n\n\n\n\n\n\n\n\nImplementing the Poission Regression\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit model\nglm_airbnb = sm.GLM(Y_airbnb, X_airbnb_np, family=sm.families.Poisson()).fit()\n\n# View summary\nprint(glm_airbnb.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30149\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2418e+05\nDate:                Mon, 12 May 2025   Deviance:                   9.2689e+05\nTime:                        09:54:12   Pearson chi2:                 1.37e+06\nNo. Iterations:                    10   Pseudo R-squ. (CS):             0.6840\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.5533      0.016    219.754      0.000       3.522       3.585\nx1            -0.1177      0.004    -31.394      0.000      -0.125      -0.110\nx2             0.0741      0.002     37.197      0.000       0.070       0.078\nx3             0.1131      0.001     75.611      0.000       0.110       0.116\nx4            -0.0769      0.002    -47.796      0.000      -0.080      -0.074\nx5            -0.0911      0.002    -50.490      0.000      -0.095      -0.088\nx6             0.3459      0.003    119.666      0.000       0.340       0.352\nx7            -0.0034      0.002     -2.151      0.031      -0.006      -0.000\nx8             0.0635      0.000    129.755      0.000       0.063       0.064\nx9            -0.0105      0.003     -3.847      0.000      -0.016      -0.005\nx10           -0.2463      0.009    -28.578      0.000      -0.263      -0.229\n==============================================================================\n\n\n\n\n\n\n\n\n\n\nAirBNB Reviews Interpretation\n\n\n\nWe modeled the number of reviews (as a proxy for bookings) using Poisson regression. Key findings include:\n\nInstant bookable listings are significantly more likely to get reviews — suggesting ease of booking matters to users.\nLarger listings (more bedrooms) and cleanliness scores are also strong positive predictors.\nShared rooms have much fewer bookings than entire homes, and Private rooms also see reduced volume.\nHigher prices slightly reduce bookings, consistent with price sensitivity.\n“Value” and “location” scores were surprisingly negative, possibly reflecting underlying price or geography-related confounders.\n\nOverall, the model helps identify which listing attributes are associated with higher demand in NYC’s Airbnb market."
  }
]