{
  "hash": "e052dba563a634062a089dc3540ed129",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Key Drivers Analysis\"\nauthor: \"Dominic Schenone\"\ndate: \"June 11 2025\"\n---\n\n\nThis post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\n\n\n\n\n#  Understanding the Landscape of Machine Learning\n\nWhen diving into machine learning, two primary branches emerge: **supervised** and **unsupervised** learning. Each serves a different purpose and is best suited for particular types of business or research questions.\n\n> **Tip**\n> Knowing when to use supervised vs unsupervised learning is crucial. It‚Äôs not just about which algorithm performs best‚Äîit's about which type of model is appropriate for the task at hand.\n\n---\n\n##  Supervised Learning: Learning from Labels\n\nSupervised learning uses *labeled data*, where the correct outcome is known. The goal is to learn a mapping from inputs (features) to outputs (labels) that can be generalized to predict future outcomes.\n\n###  Common Applications\n- Predicting **customer churn** (binary classification)\n- Forecasting **sales or prices** (regression)\n- Recognizing **objects in images** (multi-class classification)\n\n###  Algorithms\n- Linear/Logistic Regression\n- Decision Trees, Random Forests\n- Support Vector Machines\n- **K-Nearest Neighbors (KNN)** ‚Üê _we‚Äôll implement this_\n\n###  Challenges\n- Requires a **well-labeled** dataset\n- Risk of **overfitting** to training data\n- Performance is sensitive to irrelevant features or noisy labels\n\n---\n\n##  Unsupervised Learning: Finding Hidden Patterns\n\nIn contrast, unsupervised learning deals with **unlabeled data**. The algorithm tries to uncover hidden patterns, groupings, or structures from the input alone.\n\n###  Common Applications\n- **Customer segmentation** in marketing\n- **Anomaly detection** in fraud analytics\n- **Topic modeling** in NLP\n\n###  Algorithms\n- K-Means Clustering \n- Hierarchical Clustering\n- Principal Component Analysis (PCA)\n- Latent Class Models \n\n###  Challenges\n- Evaluation is **less straightforward**‚Äîno ground truth\n- Sensitive to **initialization** and **scaling**\n- Choosing the right number of clusters or components is tricky\n\n---\n\n##  Supervised vs Unsupervised: Key Differences\n\n| Aspect                  | Supervised Learning             | Unsupervised Learning             |\n|-------------------------|----------------------------------|-----------------------------------|\n| Input Data              | Labeled                         | Unlabeled                         |\n| Goal                    | Predict outcomes                | Find structure                    |\n| Evaluation              | Metrics like accuracy, RMSE     | Subjective, silhouette score, etc.|\n| Complexity              | Depends on model type           | Often computationally expensive   |\n| Common Use Cases        | Classification, Regression      | Clustering, Dimensionality Reduction |\n\n---\n\n##  Our Use Case: Data-Driven Insights Across Domains\n\nIn this assignment, we‚Äôll apply both supervised and unsupervised learning to **two unique datasets**, each offering its own opportunities and challenges.\n\n\n\n### üêß 1. **Palmer Penguins Dataset**\n- **Purpose:** Supervised learning with K-Nearest Neighbors (KNN)\n- **Task:** Classify penguins by species based on physical characteristics\n\n### üè∑Ô∏è 2. **Brand Key Drivers Dataset**\n- **Purpose:** Unsupervised learning via K-Means clustering\n- **Task:** Segment respondents by their perceptions of brand characteristics\n\n---\n\nIn the next sections, we‚Äôll explore how to prepare these datasets and apply machine learning techniques to draw actionable insights. Whether predicting outcomes or discovering hidden structures, machine learning gives us a powerful lens through which to understand the data.\n\n::: {.callout-note title=\"Load and Inspect the Penguin Dataset\"}\n\n::: {#cell-load-penguins .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npenguins = pd.read_csv(\"palmer_penguins.csv\")\npenguins_clean = penguins.dropna()  # Drop missing values\n\npenguins_clean.head()\n```\n\n::: {#load-penguins .cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181</td>\n      <td>3750</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186</td>\n      <td>3800</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195</td>\n      <td>3250</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193</td>\n      <td>3450</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.3</td>\n      <td>20.6</td>\n      <td>190</td>\n      <td>3650</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n::: {.callout-note title=\"Visualize Feature Relationships by Species\"}\n\n::: {#cell-pairplot-penguins .cell execution_count=2}\n``` {.python .cell-code}\nsns.pairplot(penguins_clean, hue=\"species\")\nplt.suptitle(\"Pairplot of Penguin Features by Species\", y=1.02)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Pairplot of Numeric Features in Penguin Dataset](hw4_questions_files/figure-html/pairplot-penguins-output-1.png){#pairplot-penguins width=1310 height=1227 fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.callout-summary title=\"Summary: Penguin Dataset Insights\"}\nThe pairplot reveals strong species-level clustering in variables like `bill_length_mm`, `flipper_length_mm`, and `body_mass_g`. These features appear highly informative and will likely support strong performance in classification models such as **K-Nearest Neighbors (KNN)**.\n:::\n\n\n::: {.callout-note title=\"Compute Correlations Between Brand Attributes\"}\n\n::: {#cell-load-drivers .cell execution_count=3}\n``` {.python .cell-code}\ndrivers = pd.read_csv(\"data_for_drivers_analysis.csv\")\n\n# Remove non-feature columns\ndrivers_corr = drivers.drop(columns=[\"id\", \"brand\"]).corr()\ndrivers_corr\n```\n\n::: {#load-drivers .cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction</th>\n      <th>trust</th>\n      <th>build</th>\n      <th>differs</th>\n      <th>easy</th>\n      <th>appealing</th>\n      <th>rewarding</th>\n      <th>popular</th>\n      <th>service</th>\n      <th>impact</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>satisfaction</th>\n      <td>1.000000</td>\n      <td>0.255706</td>\n      <td>0.191896</td>\n      <td>0.184801</td>\n      <td>0.212985</td>\n      <td>0.207997</td>\n      <td>0.194561</td>\n      <td>0.171425</td>\n      <td>0.251098</td>\n      <td>0.254539</td>\n    </tr>\n    <tr>\n      <th>trust</th>\n      <td>0.255706</td>\n      <td>1.000000</td>\n      <td>0.399652</td>\n      <td>0.306493</td>\n      <td>0.480973</td>\n      <td>0.420704</td>\n      <td>0.423868</td>\n      <td>0.389409</td>\n      <td>0.503966</td>\n      <td>0.354062</td>\n    </tr>\n    <tr>\n      <th>build</th>\n      <td>0.191896</td>\n      <td>0.399652</td>\n      <td>1.000000</td>\n      <td>0.370705</td>\n      <td>0.443953</td>\n      <td>0.369456</td>\n      <td>0.435770</td>\n      <td>0.333667</td>\n      <td>0.407956</td>\n      <td>0.383639</td>\n    </tr>\n    <tr>\n      <th>differs</th>\n      <td>0.184801</td>\n      <td>0.306493</td>\n      <td>0.370705</td>\n      <td>1.000000</td>\n      <td>0.349693</td>\n      <td>0.418159</td>\n      <td>0.349758</td>\n      <td>0.266456</td>\n      <td>0.367615</td>\n      <td>0.416957</td>\n    </tr>\n    <tr>\n      <th>easy</th>\n      <td>0.212985</td>\n      <td>0.480973</td>\n      <td>0.443953</td>\n      <td>0.349693</td>\n      <td>1.000000</td>\n      <td>0.432904</td>\n      <td>0.461316</td>\n      <td>0.387304</td>\n      <td>0.456976</td>\n      <td>0.412092</td>\n    </tr>\n    <tr>\n      <th>appealing</th>\n      <td>0.207997</td>\n      <td>0.420704</td>\n      <td>0.369456</td>\n      <td>0.418159</td>\n      <td>0.432904</td>\n      <td>1.000000</td>\n      <td>0.481159</td>\n      <td>0.376080</td>\n      <td>0.425463</td>\n      <td>0.394282</td>\n    </tr>\n    <tr>\n      <th>rewarding</th>\n      <td>0.194561</td>\n      <td>0.423868</td>\n      <td>0.435770</td>\n      <td>0.349758</td>\n      <td>0.461316</td>\n      <td>0.481159</td>\n      <td>1.000000</td>\n      <td>0.350825</td>\n      <td>0.457016</td>\n      <td>0.384245</td>\n    </tr>\n    <tr>\n      <th>popular</th>\n      <td>0.171425</td>\n      <td>0.389409</td>\n      <td>0.333667</td>\n      <td>0.266456</td>\n      <td>0.387304</td>\n      <td>0.376080</td>\n      <td>0.350825</td>\n      <td>1.000000</td>\n      <td>0.378262</td>\n      <td>0.305265</td>\n    </tr>\n    <tr>\n      <th>service</th>\n      <td>0.251098</td>\n      <td>0.503966</td>\n      <td>0.407956</td>\n      <td>0.367615</td>\n      <td>0.456976</td>\n      <td>0.425463</td>\n      <td>0.457016</td>\n      <td>0.378262</td>\n      <td>1.000000</td>\n      <td>0.412313</td>\n    </tr>\n    <tr>\n      <th>impact</th>\n      <td>0.254539</td>\n      <td>0.354062</td>\n      <td>0.383639</td>\n      <td>0.416957</td>\n      <td>0.412092</td>\n      <td>0.394282</td>\n      <td>0.384245</td>\n      <td>0.305265</td>\n      <td>0.412313</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n::: {.callout-note title=\"Visualize the Correlation Matrix with a Heatmap\"}\n\n::: {#cell-heatmap-drivers .cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\nsns.heatmap(drivers_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", center=0)\nplt.title(\"Correlation Matrix: Brand Satisfaction and Attributes\")\nplt.tight_layout()\nplt.show\n```\n\n::: {.cell-output .cell-output-display}\n![Correlation Heatmap of Brand Attributes](hw4_questions_files/figure-html/heatmap-drivers-output-1.png){#heatmap-drivers width=877 height=567 fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.callout-summary title=\"Summary: Brand Attribute Correlations\"}\nVariables such as `trust`, `appealing`, and `service` exhibit moderate positive correlation with `satisfaction`. These drivers could be critical in uncovering distinct **consumer segments** via clustering, and in predicting satisfaction using regression-style models.\n:::\n\nüß© Unsupervised Learning: K-Means Clustering on Brand Perceptions\nTo discover hidden customer segments based on brand attribute perceptions, we'll apply K-Means clustering to the key drivers dataset. This method groups observations into clusters based on similarity, helping marketers understand distinct consumer mindsets.\n\n::: {.callout-note title=\"Scale Features for K-Means Clustering\"}\n\n::: {#preprocess-drivers .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\n# Reload to ensure clean slate\nimport pandas as pd\ndrivers = pd.read_csv(\"data_for_drivers_analysis.csv\")\n\n# Use only feature columns (drop identifiers)\nX = drivers.drop(columns=[\"id\", \"brand\", \"satisfaction\"])\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n:::\n\n\n:::\n\n::: {.callout-note title=\"Use Elbow Method to Select Number of Clusters\"}\n\n::: {#cell-elbow-method .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ninertia = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    inertia.append(kmeans.inertia_)\n\nplt.plot(k_range, inertia, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Elbow Plot for K-Means Clustering](hw4_questions_files/figure-html/elbow-method-output-1.png){#elbow-method width=661 height=468 fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.callout-note title=\"Fit KMeans with Chosen k (e.g., 3)\"}\n\n::: {#cell-kmeans-cluster .cell execution_count=7}\n``` {.python .cell-code}\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Append cluster labels to original DataFrame\ndrivers['cluster'] = clusters\ndrivers[['cluster'] + list(X.columns)].groupby(\"cluster\").mean().round(2)\n```\n\n::: {#kmeans-cluster .cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>trust</th>\n      <th>build</th>\n      <th>differs</th>\n      <th>easy</th>\n      <th>appealing</th>\n      <th>rewarding</th>\n      <th>popular</th>\n      <th>service</th>\n      <th>impact</th>\n    </tr>\n    <tr>\n      <th>cluster</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.90</td>\n      <td>0.87</td>\n      <td>0.80</td>\n      <td>0.94</td>\n      <td>0.86</td>\n      <td>0.85</td>\n      <td>0.83</td>\n      <td>0.89</td>\n      <td>0.84</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.15</td>\n      <td>0.12</td>\n      <td>0.11</td>\n      <td>0.12</td>\n      <td>0.09</td>\n      <td>0.07</td>\n      <td>0.19</td>\n      <td>0.08</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.74</td>\n      <td>0.51</td>\n      <td>0.18</td>\n      <td>0.69</td>\n      <td>0.52</td>\n      <td>0.56</td>\n      <td>0.71</td>\n      <td>0.56</td>\n      <td>0.17</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n::: {.callout-note title=\"Use PCA to Visualize Customer Segments\"}\n\n::: {#cell-pca-visual .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\ndrivers['pca1'] = X_pca[:, 0]\ndrivers['pca2'] = X_pca[:, 1]\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=drivers, x=\"pca1\", y=\"pca2\", hue=\"cluster\", palette=\"Set2\", s=70)\nplt.title(\"K-Means Clustering of Brand Perceptions\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![PCA Visualization of K-Means Clusters](hw4_questions_files/figure-html/pca-visual-output-1.png){#pca-visual width=757 height=468 fig-align='center'}\n:::\n:::\n\n\n:::\n::: {.callout-summary title=\"Summary: Customer Segments from K-Means Clustering\"}\n\nK-Means clustering revealed three distinct customer segments based on how they perceive brand attributes:\n\n- **Cluster 0 ‚Äì The Enthusiasts**  \n  These respondents rate all brand dimensions‚Äî**trust, ease, appeal, service, and impact**‚Äîvery highly. They are likely loyal, satisfied users who should be nurtured through loyalty programs or brand ambassador initiatives.\n\n- **Cluster 1 ‚Äì The Selective Believers**  \n  Moderately positive on select traits (e.g., trust, service), but less impressed with differentiation and innovation. Messaging to this group should emphasize what makes the brand unique and valuable.\n\n- **Cluster 2 ‚Äì The Disengaged Skeptics**  \n  Low across the board on all brand attributes. These customers either need substantial re-engagement or may be unresponsive to marketing efforts altogether.\n\nTogether, these insights can guide targeted brand strategies and campaign design for different audience mindsets.\n\n:::\n\nüßÆ Supervised Learning: K-Nearest Neighbors with Penguin Species Classification\nIn this analysis, we‚Äôll use the Palmer Penguins dataset to train a K-Nearest Neighbors (KNN) classifier that predicts a penguin‚Äôs species based on physical characteristics like flipper length, bill depth, and body mass.\n\n::: {.callout-note title=\"Load and Preprocess the Penguin Dataset\"}\n\n::: {#load-knn-data .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load and drop missing values\npenguins = pd.read_csv(\"palmer_penguins.csv\").dropna()\n\n# Select features and target\nfeatures = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\nX = penguins[features]\ny = penguins[\"species\"]\n\n# Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n```\n:::\n\n\n:::\n::: {.callout-note title=\"Cleaning the Penguin Data\"}\nThis code loads the Palmer Penguins dataset, removes rows with missing values, and selects relevant features for classification. We split the dataset into training and test sets to evaluate model performance on unseen data, ensuring a fair and unbiased estimate.\n:::\n\n::: {.callout-note title=\"Train and Predict Using K-Nearest Neighbors\"}\n\n::: {#train-knn .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Initialize model\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = knn.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Test Accuracy: {accuracy:.2%}\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 78.00%\n              precision    recall  f1-score   support\n\n      Adelie       0.72      0.86      0.78        44\n   Chinstrap       0.70      0.35      0.47        20\n      Gentoo       0.89      0.92      0.90        36\n\n    accuracy                           0.78       100\n   macro avg       0.77      0.71      0.72       100\nweighted avg       0.78      0.78      0.76       100\n\n```\n:::\n:::\n\n\n:::\n::: {.callout-note title=\"K-Nearest Neighbor\"}\nWe train a K-Nearest Neighbors classifier with `k=5`, meaning predictions are based on the five closest neighbors in feature space. We then evaluate performance using accuracy and detailed classification metrics to understand how well the model performs on each species.\n:::\n\n::: {.callout-note title=\"Visualize the Confusion Matrix\"}\n\n::: {#cell-cmatrix .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, y_pred, labels=knn.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\n\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix: KNN on Penguin Species\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Confusion Matrix for KNN Classification](hw4_questions_files/figure-html/cmatrix-output-1.png){#cmatrix width=575 height=468 fig-align='center'}\n:::\n:::\n\n\n:::\n::: {.callout-note title=\"Don't be Confused by the Confusion Matrix!\"}\nThe confusion matrix provides a visual summary of prediction performance by showing how many observations were correctly or incorrectly classified. It highlights which species were most often confused‚Äîespecially useful for understanding class-level weaknesses.\n:::\n\n\n::: {.callout-note title=\"Project 2D Decision Boundaries with PCA\"}\n\n::: {#cell-pca-knn .cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\npca = PCA(n_components=2)\nX_proj = pca.fit_transform(X)\n\npenguins[\"pca1\"] = X_proj[:, 0]\npenguins[\"pca2\"] = X_proj[:, 1]\n\n# Predict clusters in full set\nknn_all = KNeighborsClassifier(n_neighbors=5)\nknn_all.fit(X_proj, y)\nlabels = knn_all.predict(X_proj)\n\npenguins[\"predicted_species\"] = labels\n\nsns.scatterplot(data=penguins, x=\"pca1\", y=\"pca2\", hue=\"predicted_species\", palette=\"Set2\")\nplt.title(\"KNN Classification with PCA Projection\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![PCA Projection of KNN Classifier](hw4_questions_files/figure-html/pca-knn-output-1.png){#pca-knn width=661 height=468 fig-align='center'}\n:::\n:::\n\n\n:::\n::: {.callout-note title=\"PCA 2D Visualization\"}\nWe use PCA to reduce the feature space to two dimensions for visualization. This allows us to plot the decision boundaries and predicted species clusters in 2D, making it easier to interpret how the KNN algorithm separates the classes.\n:::\n\n::: {.callout-summary title=\"Summary: Penguin Species Classification with KNN\"}\n\nOur K-Nearest Neighbors (KNN) model achieved an overall **accuracy of 78%** in predicting penguin species based on physical measurements like bill length, flipper length, and body mass.\n\nKey insights:\n\n- **Adelie penguins** were classified well with high recall (0.86), though a few were confused with Gentoo.\n- **Gentoo penguins** had the best precision and recall (F1-score: 0.90), showing that their physical traits are most distinct.\n- **Chinstrap penguins** were the most challenging to classify, with a recall of only 0.35‚Äîoften mistaken for Adelie.\n\nThe **confusion matrix** confirms this pattern, with most errors occurring between Adelie and Chinstrap species.\n\nUsing **PCA for visualization**, we saw Gentoo penguins form a distinct cluster, while Adelie and Chinstrap overlap more‚Äîexplaining the classifier's difficulty.\n\nOverall, this analysis shows that **KNN is an effective but interpretable baseline model** for biological classification problems, especially when species have clearly separable physical traits.\n\n:::\n\n## üßæ Conclusion: What We Learned from the Data\n\nThis project explored both **unsupervised** and **supervised** machine learning techniques to derive insight from real-world marketing and scientific datasets.\n\n### üìä K-Means Clustering (Unsupervised Learning)\n\nUsing brand perception data, K-Means revealed **three distinct customer segments**:\n\n- **Enthusiasts** who love the brand across all dimensions\n- **Selective Believers** who value trust but want more differentiation\n- **Disengaged Skeptics** with minimal brand attachment\n\nThese segments offer clear targets for **marketing strategy**, **product messaging**, and **resource allocation**.\n\n### üêß K-Nearest Neighbors (Supervised Learning)\n\nKNN was applied to predict penguin species based on physical traits. The model:\n\n- Achieved **78% accuracy** on a held-out test set\n- Performed best on **Gentoo** and **Adelie** species\n- Struggled with **Chinstrap**, where features overlap with others\n\nThis demonstrates KNN‚Äôs utility as an **interpretable baseline classifier**‚Äîespecially when decision boundaries are nonlinear but structured.\n\n---\n\n### üí° Final Takeaways\n\n> - Unsupervised learning is ideal for discovering *hidden patterns* when no outcome labels exist.\n> - Supervised learning shines when *predictions* are needed from labeled data.\n> - Choosing the right tool depends on your goal: explore or predict, segment or classify.\n\nIn marketing analytics, both approaches offer value‚Äîwhether it's understanding your audience **before launch**, or **optimizing messaging** once you‚Äôve collected results.\n\n:::\n\n",
    "supporting": [
      "hw4_questions_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}